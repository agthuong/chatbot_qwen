from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
import pandas as pd
import json5
import os
import datetime
import json
import time
from fuzzywuzzy import fuzz  # Th√™m th∆∞ vi·ªán so s√°nh m·ªù

# üîß Import tool ƒë·ªãnh nghƒ©a ·ªü file kh√°c n·∫øu c√≥
# V√≠ d·ª•: from tools import csv_tool

@register_tool('tra_cuu_quy_trinh')
class TraCuuQuyTrinhTool(BaseTool):
    description = 'Tra c·ª©u th√¥ng tin quy tr√¨nh c√¥ng vi·ªác t·ª´ CSDL. Cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ c√°c quy tr√¨nh c·ªßa c√¥ng ty.'
    parameters = [{
        'name': 'query',
        'type': 'string',
        'description': 'C√¢u h·ªèi v·ªÅ quy tr√¨nh c·∫ßn t√¨m. C√≥ th·ªÉ bao g·ªìm t·ª´ kh√≥a v·ªÅ giai ƒëo·∫°n, ph√≤ng ban, c√¥ng vi·ªác, ng∆∞·ªùi th·ª±c hi·ªán.',
        'required': True
    }]

    def __init__(self, *args, **kwargs):
        super().__init__()
        import pandas as pd
        import numpy as np
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import re
        import unidecode  # Th√™m th∆∞ vi·ªán x·ª≠ l√Ω d·∫•u ti·∫øng Vi·ªát
        
        # T·∫£i d·ªØ li·ªáu
        self.df = pd.read_csv('./data.csv')
        
        # In th√¥ng tin d·∫°ng d·ªØ li·ªáu ƒë·ªÉ debug
        print("T√™n c√°c c·ªôt trong file CSV:", self.df.columns.tolist())
        
        # Ki·ªÉm tra xem c√≥ c·ªôt 'Unnamed' n√†o kh√¥ng
        has_unnamed_columns = any('Unnamed' in col for col in self.df.columns)
        
        # Ph√¢n t√≠ch xem file CSV s·ª≠ d·ª•ng t√™n c·ªôt th·∫≠t hay Unnamed
        if has_unnamed_columns:
            # Ph∆∞∆°ng ph√°p c≈© n·∫øu c√≥ c·ªôt Unnamed
            self.use_unnamed_columns = True
            try:
                header_row = self.df.iloc[1]
                actual_headers = {}
                
                # L·∫•y t√™n c·ªôt th·ª±c t·∫ø t·ª´ d√≤ng header
                for i, col_name in enumerate(self.df.columns):
                    if i < len(header_row) and isinstance(header_row[i], str) and header_row[i].strip():
                        actual_headers[header_row[i]] = col_name
                        print(f"Header th·ª±c t·∫ø: '{header_row[i]}' ·ªü c·ªôt '{col_name}'")
                
                # In th√¥ng tin v·ªÅ header ƒë√£ ph√¢n t√≠ch
                print(f"ƒê√£ ph√¢n t√≠ch ƒë∆∞·ª£c {len(actual_headers)} header th·ª±c t·∫ø t·ª´ CSV")
            except Exception as e:
                print(f"L·ªói khi ph√¢n t√≠ch header: {e}")
                actual_headers = {}
                
            # Ki·ªÉm tra v√† lo·∫°i b·ªè c√°c h√†ng ti√™u ƒë·ªÅ/metadata 
            first_rows = self.df.iloc[:5]
            is_header = []
            
            for i, row in first_rows.iterrows():
                header_like_count = sum(1 for x in row if isinstance(x, str) and 
                    (x.istitle() or x.isupper() or x in ['A', 'R'] or 
                     any(term in x for term in ['Giai ƒëo·∫°n', 'C√¥ng vi·ªác', 'Ph√≤ng ban', 'M·ª•c ti√™u'])))
                is_header.append(header_like_count > 3)
            
            # T·∫°o danh s√°ch c√°c ch·ªâ s·ªë c·∫ßn lo·∫°i b·ªè
            drop_indices = [i for i, is_h in enumerate(is_header) if is_h and i < 5]
            
            # Lo·∫°i b·ªè c√°c h√†ng header b·∫±ng index
            if drop_indices:
                self.df = self.df.drop(drop_indices).reset_index(drop=True)
                
            # C·∫≠p nh·∫≠t √°nh x·∫° t√™n c·ªôt d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø trong file CSV
            self.column_names = {
                'Giai ƒëo·∫°n': actual_headers.get('Giai ƒëo·∫°n', 'Unnamed: 1'),
                'C√¥ng vi·ªác': actual_headers.get('C√¥ng vi·ªác', 'Unnamed: 2'),
                'Ph√≤ng ban': actual_headers.get('Ph√≤ng ban', 'Unnamed: 3'),
                'ƒêi·ªÅu ki·ªán ti√™n quy·∫øt': actual_headers.get('ƒêi·ªÅu ki·ªán ti√™n quy·∫øt', 'Unnamed: 4'),
                'Ng∆∞·ªùi ph·ª• tr√°ch': actual_headers.get('A', 'Unnamed: 5'),  # C·ªôt A
                'Ng∆∞·ªùi th·ª±c hi·ªán': actual_headers.get('R', 'Unnamed: 6'),  # C·ªôt R
                'L√†m g√¨': actual_headers.get('L√†m g√¨', 'Unnamed: 7'),
                'M·ª•c ti√™u': actual_headers.get('M·ª•c ti√™u', 'Unnamed: 8'),
                'K·∫øt qu·∫£ tr·∫£ ra': actual_headers.get('K·∫øt qu·∫£ tr·∫£ ra', 'Unnamed: 9'),
                'Duration': actual_headers.get('Duration', 'Unnamed: 10'),
                'Receiver': actual_headers.get('Receiver', 'Unnamed: 11'),
                'ƒê·ªãnh k·ª≥ th·ª±c hi·ªán': actual_headers.get('ƒê·ªãnh k·ª≥ th·ª±c hi·ªán', 'Unnamed: 12'),
                'G·ª≠i k·∫øt qu·∫£ qua': actual_headers.get('G·ª≠i k·∫øt qu·∫£ qua', 'Unnamed: 13'),
                'Form th·ª±c hi·ªán': actual_headers.get('Form th·ª±c hi·ªán', 'Unnamed: 14'),
                'ƒêo l∆∞·ªùng/ƒë√°nh gi√°': actual_headers.get('ƒêo l∆∞·ªùng/ƒë√°nh gi√°', 'Unnamed: 15')
            }
        else:
            # Ph∆∞∆°ng ph√°p m·ªõi n·∫øu c√≥ t√™n c·ªôt th·∫≠t
            self.use_unnamed_columns = False
            # L·ªçc d·ªØ li·ªáu tr·ªëng
            # Ki·ªÉm tra xem d√≤ng ƒë·∫ßu ti√™n c√≥ ph·∫£i l√† header kh√¥ng
            if pd.isna(self.df.iloc[0]['Giai ƒëo·∫°n']) or self.df.iloc[0]['Giai ƒëo·∫°n'] == '':
                self.df = self.df.iloc[1:].reset_index(drop=True)
                
            # √Ånh x·∫° tr·ª±c ti·∫øp t√™n c·ªôt
            self.column_names = {
                'Giai ƒëo·∫°n': 'Giai ƒëo·∫°n',
                'C√¥ng vi·ªác': 'C√¥ng vi·ªác',
                'Ph√≤ng ban': 'Ph√≤ng ban',
                'ƒêi·ªÅu ki·ªán ti√™n quy·∫øt': 'ƒêi·ªÅu ki·ªán ti√™n quy·∫øt',
                'Ng∆∞·ªùi ph·ª• tr√°ch': 'A',  # C·ªôt A
                'Ng∆∞·ªùi th·ª±c hi·ªán': 'R',  # C·ªôt R
                'L√†m g√¨': 'L√†m g√¨',
                'M·ª•c ti√™u': 'M·ª•c ti√™u',
                'K·∫øt qu·∫£ tr·∫£ ra': 'K·∫øt qu·∫£ tr·∫£ ra',
                'Duration': 'Duration',
                'Receiver': 'Receiver',
                'ƒê·ªãnh k·ª≥ th·ª±c hi·ªán': 'ƒê·ªãnh k·ª≥ th·ª±c hi·ªán',
                'G·ª≠i k·∫øt qu·∫£ qua': 'G·ª≠i k·∫øt qu·∫£ qua',
                'Form th·ª±c hi·ªán': 'Form th·ª±c hi·ªán',
                'ƒêo l∆∞·ªùng/ƒë√°nh gi√°': 'ƒêo l∆∞·ªùng/ƒë√°nh gi√°'
            }
            
        # L·ªçc d·ªØ li·ªáu tr·ªëng m·ªôt c√°ch ch·∫∑t ch·∫Ω h∆°n
        content_mask = np.array([
            sum(1 for x in row if pd.notna(x) and str(x).strip() != '') > 3 
            for _, row in self.df.iterrows()
        ])
        
        self.df = self.df[content_mask].reset_index(drop=True)
        
        # S·ª≠a l·ªói fillna
        for col in self.df.columns:
            if self.df[col].dtype == 'float64' or self.df[col].dtype == 'int64':
                self.df[col].fillna(0, inplace=True)
            else:
                self.df[col].fillna('', inplace=True)
        
        # In ra th·ªëng k√™ ƒë·ªÉ ki·ªÉm tra
        print(f"D·ªØ li·ªáu sau khi l·ªçc: {len(self.df)} h√†ng, {len(self.df.columns)} c·ªôt")
        
        # In ra √°nh x·∫° t√™n c·ªôt ƒë·ªÉ ki·ªÉm tra
        print("√Ånh x·∫° t√™n c·ªôt:")
        for friendly_name, col_name in self.column_names.items():
            print(f"  {friendly_name} -> {col_name}")
        
        # T·∫°o t·ª´ ƒëi·ªÉn map ng∆∞·ª£c l·∫°i ƒë·ªÉ ti·ªán s·ª≠ d·ª•ng
        self.inverse_column_names = {v: k for k, v in self.column_names.items()}
        
        # H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (l√†m s·∫°ch v√† chu·∫©n h√≥a)
        def preprocess_text(text):
            if not isinstance(text, str):
                return ""
            # Chuy·ªÉn sang ch·ªØ th∆∞·ªùng
            text = text.lower()
            # Lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
            text = re.sub(r'[^\w\s]', ' ', text)
            # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            text = re.sub(r'\s+', ' ', text).strip()
            # T·∫°o phi√™n b·∫£n kh√¥ng d·∫•u ƒë·ªÉ h·ªó tr·ª£ t√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát d·∫•u
            text_no_accent = unidecode.unidecode(text)
            # K·∫øt h·ª£p c·∫£ vƒÉn b·∫£n g·ªëc v√† kh√¥ng d·∫•u
            return text + " " + text_no_accent
        
        # L∆∞u l·∫°i h√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y
        self.preprocess_text = preprocess_text
        
        # Danh s√°ch stopwords ti·∫øng Vi·ªát m·ªü r·ªông
        self.vietnamese_stopwords = [
            'v√†', 'ho·∫∑c', 'c·ªßa', 'l√†', 'ƒë·ªÉ', 'trong', 'ngo√†i', 'khi', 'v·ªõi', 'b·ªüi', 
            'ƒë∆∞·ª£c', 'kh√¥ng', 'c√≥', 'n√†y', 'ƒë√≥', 'c√°c', 'nh·ªØng', 'm√†', 'n√™n', 'v√¨',
            'tr√™n', 'd∆∞·ªõi', 't·∫°i', 't·ª´', 'qua', 'm·ªôt', 'hai', 'ba', 'b·ªën', 'nƒÉm',
            's√°u', 'b·∫£y', 't√°m', 'ch√≠n', 'm∆∞·ªùi', 'r·ªìi', 's·∫Ω', 'ƒëang', 'ƒë√£', 's·∫Øp',
            'ai', 't√¥i', 'b·∫°n', 'h·ªç', 'ch√∫ng', 'ch√∫ng ta', 'm√¨nh', 'chi·∫øc', 'c√°i'
        ]
        
        # X√¢y d·ª±ng √°nh x·∫° vai tr√≤ - giai ƒëo·∫°n
        self.build_role_stage_mapping()
        
        # T·∫°o c√°c tr∆∞·ªùng t√¨m ki·∫øm theo ch·ªß ƒë·ªÅ
        self.build_search_fields()
        
    def build_role_stage_mapping(self):
        """X√¢y d·ª±ng √°nh x·∫° t·ª± ƒë·ªông gi·ªØa ng∆∞·ªùi th·ª±c hi·ªán v√† giai ƒëo·∫°n"""
        role_stage_map = {}
        
        for idx, row in self.df.iterrows():
            role_col = self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']
            stage_col = self.column_names['Giai ƒëo·∫°n']
            
            if (isinstance(row[role_col], str) and row[role_col].strip() and 
                isinstance(row[stage_col], str) and row[stage_col].strip()):
                
                # T√°ch c√°c vai tr√≤ n·∫øu c√≥ nhi·ªÅu (ph√¢n t√°ch b·ªüi d·∫•u ph·∫©y)
                roles = [r.strip().lower() for r in row[role_col].split(',')]
                stage = row[stage_col].strip()
                
                for role in roles:
                    if role not in role_stage_map:
                        role_stage_map[role] = []
                    if stage not in role_stage_map[role]:
                        role_stage_map[role].append(stage)
        
        self.role_stage_mapping = role_stage_map
        
        # In ra ƒë·ªÉ ki·ªÉm tra
        print(f"ƒê√£ x√¢y d·ª±ng √°nh x·∫° vai tr√≤ - giai ƒëo·∫°n cho {len(role_stage_map)} vai tr√≤.")

    def build_search_fields(self):
        """T·∫°o c√°c tr∆∞·ªùng t√¨m ki·∫øm theo ng·ªØ nghƒ©a v√† ch·ªâ m·ª•c vector t∆∞∆°ng ·ª©ng cho t·∫•t c·∫£ c·ªôt"""
        import numpy as np
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # T·∫°o c√°c tr∆∞·ªùng t√¨m ki·∫øm ch√≠nh
        # 1. Tr∆∞·ªùng nh√¢n s·ª±
        self.df['person_fields'] = self.df.apply(
            lambda row: ' '.join([
                str(row[self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']]), 
                str(row[self.column_names['Ng∆∞·ªùi ph·ª• tr√°ch']])
            ]), axis=1).apply(self.preprocess_text)
        
        # 2. Tr∆∞·ªùng quy tr√¨nh
        self.df['process_fields'] = self.df.apply(
            lambda row: ' '.join([
                str(row[self.column_names['Giai ƒëo·∫°n']]), 
                str(row[self.column_names['C√¥ng vi·ªác']]), 
                str(row[self.column_names['Ph√≤ng ban']])
            ]), axis=1).apply(self.preprocess_text)
        
        # 3. Tr∆∞·ªùng n·ªôi dung
        self.df['content_fields'] = self.df.apply(
            lambda row: ' '.join([
                str(row[self.column_names['M·ª•c ti√™u']]), 
                str(row[self.column_names['L√†m g√¨']]), 
                str(row[self.column_names['K·∫øt qu·∫£ tr·∫£ ra']])
            ]), axis=1).apply(self.preprocess_text)
            
        # 4. Tr∆∞·ªùng th·ªùi gian v√† t·∫ßn su·∫•t
        self.df['time_fields'] = self.df.apply(
            lambda row: ' '.join([
                str(row[self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']]), 
                str(row[self.column_names['Duration']])
            ]), axis=1).apply(self.preprocess_text)
        
        # 5. Tr∆∞·ªùng th√¥ng tin b·ªï sung
        self.df['additional_fields'] = self.df.apply(
            lambda row: ' '.join([
                str(row[self.column_names.get('ƒêi·ªÅu ki·ªán ti√™n quy·∫øt', '')]),
                str(row[self.column_names.get('Receiver', '')]),
                str(row[self.column_names.get('G·ª≠i k·∫øt qu·∫£ qua', '')]),
                str(row[self.column_names.get('Form th·ª±c hi·ªán', '')]),
                str(row[self.column_names.get('ƒêo l∆∞·ªùng/ƒë√°nh gi√°', '')])
            ]), axis=1).apply(self.preprocess_text)
        
        # T·∫°o tr∆∞·ªùng t√¨m ki·∫øm t·ªïng h·ª£p c√≥ tr·ªçng s·ªë
        self.df['search_text'] = ''
        
        # Nh√¢n tr·ªçng s·ªë cho c√°c tr∆∞·ªùng kh√°c nhau
        field_weights = {
            'process_fields': 3,    # Tr·ªçng s·ªë cao nh·∫•t cho th√¥ng tin quy tr√¨nh
            'content_fields': 2,    # Tr·ªçng s·ªë cao cho n·ªôi dung
            'person_fields': 1,     # Tr·ªçng s·ªë b√¨nh th∆∞·ªùng cho ng∆∞·ªùi th·ª±c hi·ªán
            'time_fields': 2,       # TƒÉng tr·ªçng s·ªë cho th√¥ng tin th·ªùi gian
            'additional_fields': 1  # Tr·ªçng s·ªë th·∫•p cho th√¥ng tin b·ªï sung
        }
        
        for field, weight in field_weights.items():
            weighted_text = self.df[field].apply(lambda x: ' '.join([x] * weight))
            self.df['search_text'] += ' ' + weighted_text
        
        # Vector h√≥a tr∆∞·ªùng nh√¢n s·ª±
        self.person_vectorizer = TfidfVectorizer(
            lowercase=True,
            max_df=0.9, min_df=1, 
            ngram_range=(1, 2))
        self.person_matrix = self.person_vectorizer.fit_transform(self.df['person_fields'])
        
        # Vector h√≥a tr∆∞·ªùng quy tr√¨nh
        self.process_vectorizer = TfidfVectorizer(
            lowercase=True, 
            max_df=0.9, min_df=1, 
            ngram_range=(1, 3))
        self.process_matrix = self.process_vectorizer.fit_transform(self.df['process_fields'])
        
        # Vector h√≥a tr∆∞·ªùng n·ªôi dung
        self.content_vectorizer = TfidfVectorizer(
            lowercase=True, 
            max_df=0.9, min_df=1, 
            ngram_range=(1, 3),
            stop_words=self.vietnamese_stopwords)
        self.content_matrix = self.content_vectorizer.fit_transform(self.df['content_fields'])
        
        # Vector h√≥a tr∆∞·ªùng th·ªùi gian
        self.time_vectorizer = TfidfVectorizer(
            lowercase=True, 
            max_df=0.9, min_df=1, 
            ngram_range=(1, 2))
        self.time_matrix = self.time_vectorizer.fit_transform(self.df['time_fields'])
        
        # Vector h√≥a tr∆∞·ªùng th√¥ng tin b·ªï sung
        self.additional_vectorizer = TfidfVectorizer(
            lowercase=True, 
            max_df=0.9, min_df=1, 
            ngram_range=(1, 2))
        self.additional_matrix = self.additional_vectorizer.fit_transform(self.df['additional_fields'])
        
        # Vector h√≥a t·ª´ng c·ªôt ri√™ng l·∫ª ƒë·ªÉ t√¨m ki·∫øm ch√≠nh x√°c
        self.column_matrices = {}
        self.column_vectorizers = {}
        
        # T·∫°o vector cho t·ª´ng c·ªôt trong CSV ƒë·ªÉ t√¨m ki·∫øm ch√≠nh x√°c
        for col_name, actual_col in self.column_names.items():
            if actual_col in self.df.columns:
                # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n trong c·ªôt
                processed_col_text = self.df[actual_col].astype(str).apply(self.preprocess_text)
                
                # T·∫°o vectorizer cho c·ªôt
                vectorizer = TfidfVectorizer(
                    lowercase=True,
                    max_df=0.95, min_df=1,
                    ngram_range=(1, 2))
                
                try:
                    # T·∫°o ma tr·∫≠n vector cho c·ªôt
                    matrix = vectorizer.fit_transform(processed_col_text)
                    
                    # L∆∞u tr·ªØ c·∫£ vectorizer v√† ma tr·∫≠n
                    self.column_vectorizers[col_name] = vectorizer
                    self.column_matrices[col_name] = matrix
                    
                    print(f"ƒê√£ t·∫°o ma tr·∫≠n vector cho c·ªôt '{col_name}'")
                except Exception as e:
                    print(f"L·ªói khi vector h√≥a c·ªôt '{col_name}': {e}")
        
        # Vector h√≥a tr∆∞·ªùng t·ªïng h·ª£p cho t√¨m ki·∫øm m·∫∑c ƒë·ªãnh
        self.main_vectorizer = TfidfVectorizer(
            lowercase=True,
            ngram_range=(1, 3),
            stop_words=self.vietnamese_stopwords,
            max_features=10000,
            min_df=1,
            max_df=0.95,
            token_pattern=r'\b\w+\b'
        )
        self.main_matrix = self.main_vectorizer.fit_transform(self.df['search_text'])

    def call(self, params: str, **kwargs):
        """T√¨m ki·∫øm quy tr√¨nh d·ª±a tr√™n query"""
        import pandas as pd
        import numpy as np
        import json
        import json5
        from fuzzywuzzy import fuzz
        
        # Ph√¢n t√≠ch tham s·ªë ƒë·∫ßu v√†o
        try:
            # N·∫øu params ƒë√£ l√† chu·ªói query, d√πng tr·ª±c ti·∫øp
            if isinstance(params, str) and not params.startswith('{'):
                query = params
            # N·∫øu params l√† JSON string, parse n√≥
            else:
                query_data = json5.loads(params)
                query = query_data['query']
        except Exception as e:
            # Fallback: n·∫øu c√≥ l·ªói khi parse JSON, coi params l√† query tr·ª±c ti·∫øp
            print(f"L·ªói khi x·ª≠ l√Ω tham s·ªë: {e}, d√πng params l√†m query.")
            query = str(params)
        
        # Ph√¢n t√≠ch truy v·∫•n
        query_info = self.analyze_query(query)
        
        # Thi·∫øt l·∫≠p s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
        max_results = 10 if 'time_focus' in query_info['components'] else 3
        
        # T√¨m ki·∫øm ƒëa chi·ªÅu d·ª±a tr√™n lo·∫°i truy v·∫•n
        search_results = self.multi_vector_search(query_info, top_n=max_results)
        
        # ƒê·∫∑c bi·ªát x·ª≠ l√Ω truy v·∫•n v·ªÅ th·ªùi gian
        if ('time_focus' in query_info['components'] and not search_results) or ('dinh_ky' in query_info['components']):
            # T√¨m ki·∫øm tr·ª±c ti·∫øp theo ƒë·ªãnh k·ª≥ th·ª±c hi·ªán
            time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
            direct_results = []
            time_unit = query_info['components'].get('time_unit', '')
            
            # N·∫øu c√≥ ƒë∆°n v·ªã th·ªùi gian c·ª• th·ªÉ, t√¨m tr·ª±c ti·∫øp
            if time_unit:
                print(f"T√¨m ki·∫øm tr·ª±c ti·∫øp v·ªõi ƒë∆°n v·ªã th·ªùi gian: {time_unit}")
                for idx in range(len(self.df)):
                    if isinstance(self.df.iloc[idx][time_col], str) and time_unit in self.df.iloc[idx][time_col].lower():
                        # T√≠nh ƒëi·ªÉm s·ªë c∆° b·∫£n c·ªông th√™m ƒëi·ªÉm cho s·ªë t·ª´ kh√≥a kh·ªõp
                        keyword_matches = sum(1 for kw in query_info['components']['keywords'] 
                                             if kw in str(self.df.iloc[idx]).lower())
                        score = 0.6 + min(0.3, keyword_matches * 0.05)
                        direct_results.append((idx, score))
                
                if direct_results:
                    # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë
                    direct_results = sorted(direct_results, key=lambda x: x[1], reverse=True)
                    print(f"T√¨m ƒë∆∞·ª£c {len(direct_results)} k·∫øt qu·∫£ tr·ª±c ti·∫øp cho '{time_unit}'")
                    # Tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c thay v√¨ ch·ªâ l·∫•y top 3
                    search_results = direct_results[:max_results]
        
        # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, tr·∫£ v·ªÅ th√¥ng b√°o
        if not search_results:
            suggestions = []
            
            # ƒê·ªÅ xu·∫•t d·ª±a tr√™n lo·∫°i truy v·∫•n
            if 'time_focus' in query_info['components']:
                time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
                # L·∫•y c√°c gi√° tr·ªã ƒë·ªãnh k·ª≥ th·ª±c t·∫ø t·ª´ d·ªØ li·ªáu
                unique_periods = [p for p in self.df[time_col].dropna().unique() 
                                 if isinstance(p, str) and p.strip()]
                
                if unique_periods:
                    period_examples = ", ".join(unique_periods[:3])
                    suggestions.append(f"th·ª≠ t√¨m v·ªõi ƒë·ªãnh k·ª≥ c·ª• th·ªÉ nh∆∞: {period_examples}")
            
            return {
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p',
                'query_info': query_info,
                'suggestions': suggestions,
                'results': []
            }
        
        # L·ªçc b·ªè c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm s·ªë qu√° th·∫•p
        min_threshold = 0.15
        filtered_results = [(idx, score) for idx, score in search_results if score > min_threshold]
        
        # N·∫øu kh√¥ng c√≤n k·∫øt qu·∫£ sau khi l·ªçc, tr·∫£ v·ªÅ th√¥ng b√°o
        if not filtered_results:
            return {
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p ho·∫∑c ƒë·ªô t∆∞∆°ng ƒë·ªìng qu√° th·∫•p',
                'query_info': query_info,
                'results': []
            }
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i l√† truy v·∫•n v·ªÅ vai tr√≤ v√† giai ƒëo·∫°n kh√¥ng
        is_role_stage_query = False
        role = None
        expected_stages = []
        
        if 'nguoi_thuc_hien' in query_info['components'] and query_info['components']['nguoi_thuc_hien']:
            role = query_info['components']['nguoi_thuc_hien'].lower()
            if role in self.role_stage_mapping:
                expected_stages = self.role_stage_mapping[role]
                is_role_stage_query = any(term in query.lower() for term in ['thu·ªôc', 'giai ƒëo·∫°n', 'l√†m vi·ªác'])
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i l√† truy v·∫•n v·ªÅ th·ªùi gian kh√¥ng
        is_time_query = 'time_focus' in query_info['components']
        time_unit = query_info['components'].get('time_unit', '')
        
        # Chu·∫©n b·ªã k·∫øt qu·∫£ tr·∫£ v·ªÅ
        results = []
        already_included_stages = set()  # ƒê·ªÉ tr√°nh tr√πng l·∫∑p giai ƒëo·∫°n
        
        role_col = self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']
        stage_col = self.column_names['Giai ƒëo·∫°n']
        time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
        
        for idx, score in filtered_results:
            row = self.df.iloc[idx]
            
            # Ki·ªÉm tra v√† lo·∫°i b·ªè c√°c k·∫øt qu·∫£ n·∫øu l√† ti√™u ƒë·ªÅ ho·∫∑c h√†ng header
            is_header = False
            row_text = ' '.join([str(v) for v in row.values if isinstance(v, str)])
            header_patterns = ['quy tr√¨nh', 'giai ƒëo·∫°n', 'ph√≤ng ban', 'ng∆∞·ªùi th·ª±c hi·ªán', 'm·ª•c ti√™u', 'output']
            if sum(1 for pattern in header_patterns if pattern.lower() in row_text.lower()) >= 3:
                is_header = True
            
            # X·ª≠ l√Ω truy v·∫•n v·ªÅ vai tr√≤ thu·ªôc giai ƒëo·∫°n n√†o
            if is_role_stage_query and role:
                # N·∫øu h√†ng kh√¥ng c√≥ vai tr√≤ ph√π h·ª£p, b·ªè qua
                if not (isinstance(row[role_col], str) and role in row[role_col].lower()):
                    if score < 0.4:  # V·∫´n gi·ªØ l·∫°i c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm cao
                        continue
                
                # N·∫øu l√† truy v·∫•n v·ªÅ giai ƒëo·∫°n, ∆∞u ti√™n hi·ªÉn th·ªã c√°c giai ƒëo·∫°n kh√°c nhau
                if isinstance(row[stage_col], str) and row[stage_col].strip():
                    stage = row[stage_col].strip().lower()
                    if stage in already_included_stages and score < 0.5:
                        continue
                    already_included_stages.add(stage)
                
                # TƒÉng ƒëi·ªÉm cho c√°c k·∫øt qu·∫£ c√≥ giai ƒëo·∫°n d·ª± ki·∫øn
                if isinstance(row[stage_col], str) and any(exp_stage.lower() in row[stage_col].lower() for exp_stage in expected_stages):
                    score *= 1.2  # TƒÉng ƒëi·ªÉm th√™m 20%
            
            # X·ª≠ l√Ω truy v·∫•n v·ªÅ th·ªùi gian
            if is_time_query and time_unit:
                # ∆Øu ti√™n c√°c k·∫øt qu·∫£ c√≥ th·ªùi gian ph√π h·ª£p
                if not (isinstance(row[time_col], str) and time_unit.lower() in row[time_col].lower()):
                    if score < 0.5:  # V·∫´n gi·ªØ l·∫°i c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm cao
                        continue
                else:
                    # TƒÉng ƒëi·ªÉm cho k·∫øt qu·∫£ ph√π h·ª£p v·ªõi ƒë∆°n v·ªã th·ªùi gian
                    score *= 1.3  # TƒÉng ƒëi·ªÉm th√™m 30%
            
            # B·ªè qua n·∫øu l√† header ho·∫∑c t·∫•t c·∫£ c√°c c·ªôt ƒë·ªÅu tr·ªëng
            if is_header or pd.isnull(row).all():
                continue
                
            # T·∫°o t·ª´ ƒëi·ªÉn k·∫øt qu·∫£ s·ª≠ d·ª•ng √°nh x·∫° t√™n c·ªôt thay v√¨ hard-coding
            result = {'score': score}
            
            # Th√™m c√°c tr∆∞·ªùng d·ªØ li·ªáu c√≥ √Ω nghƒ©a, s·ª≠ d·ª•ng t√™n c·ªôt th√¢n thi·ªán
            for col, value in row.items():
                if pd.notna(value) and str(value).strip():
                    # S·ª≠ d·ª•ng t√™n th√¢n thi·ªán n·∫øu c√≥, n·∫øu kh√¥ng th√¨ gi·ªØ nguy√™n t√™n c·ªôt
                    friendly_name = self.inverse_column_names.get(col, col)
                    # Chuy·ªÉn ƒë·ªïi t√™n tr∆∞·ªùng sang d·∫°ng snake_case ƒë·ªÉ d·ªÖ s·ª≠ d·ª•ng
                    field_name = friendly_name.lower().replace(' ', '_').replace('/', '_')
                    result[field_name] = value
            
            # N·∫øu ch∆∞a c√≥ giai ƒëo·∫°n nh∆∞ng c√≥ th·ªÉ suy ra t·ª´ c√°c h√†ng tr∆∞·ªõc
            if 'giai_ƒëo·∫°n' not in result or not result['giai_ƒëo·∫°n'] and idx > 0:
                # T√¨m trong c√°c h√†ng tr√™n ƒë·ªÉ l·∫•y th√¥ng tin giai ƒëo·∫°n
                for i in range(idx-1, -1, -1):
                    prev_stage = self.df.iloc[i][stage_col]
                    if isinstance(prev_stage, str) and prev_stage.strip():
                        result['giai_ƒëo·∫°n'] = prev_stage
                        break
            
            # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng quan tr·ªçng lu√¥n c√≥ m·∫∑t trong k·∫øt qu·∫£
            essential_fields = ['giai_ƒëo·∫°n', 'c√¥ng_vi·ªác', 'ph√≤ng_ban', 'ng∆∞·ªùi_th·ª±c_hi·ªán', 'm·ª•c_ti√™u']
            for field in essential_fields:
                if field not in result:
                    result[field] = ''
            
            results.append(result)
        
        # N·∫øu l√† truy v·∫•n v·ªÅ vai tr√≤ thu·ªôc giai ƒëo·∫°n n√†o v√† kh√¥ng c√≥ k·∫øt qu·∫£ c·ª• th·ªÉ
        # Tr·∫£ v·ªÅ c√°c giai ƒëo·∫°n t·ª´ b·∫£n ƒë·ªì √°nh x·∫°
        if is_role_stage_query and role and not results:
            if expected_stages:
                for stage in expected_stages:
                    result = {
                        'giai_ƒëo·∫°n': stage,
                        'ng∆∞·ªùi_th·ª±c_hi·ªán': role.capitalize(),
                        'score': 0.7  # ƒêi·ªÉm m·∫∑c ƒë·ªãnh cho k·∫øt qu·∫£ t·ª´ √°nh x·∫°
                    }
                    results.append(result)
        
        # N·∫øu l√† truy v·∫•n v·ªÅ th·ªùi gian v√† kh√¥ng c√≥ k·∫øt qu·∫£ c·ª• th·ªÉ
        if is_time_query and time_unit and not results:
            # T√¨m ki·∫øm tr·ª±c ti·∫øp m·ªôt l·∫ßn n·ªØa v·ªõi ng∆∞·ª°ng th·∫•p h∆°n
            direct_time_results = []
            for idx in range(len(self.df)):
                if isinstance(self.df.iloc[idx][time_col], str) and time_unit.lower() in self.df.iloc[idx][time_col].lower():
                    row = self.df.iloc[idx].to_dict()
                    # Ch·ªâ l·∫•y nh·ªØng tr∆∞·ªùng kh√¥ng tr·ªëng
                    result = {
                        'score': 0.6,
                        'ƒë·ªãnh_k·ª≥_th·ª±c_hi·ªán': self.df.iloc[idx][time_col]
                    }
                    
                    # Th√™m c√°c tr∆∞·ªùng quan tr·ªçng
                    for field, col_name in [
                        ('giai_ƒëo·∫°n', self.column_names['Giai ƒëo·∫°n']),
                        ('c√¥ng_vi·ªác', self.column_names['C√¥ng vi·ªác']),
                        ('ph√≤ng_ban', self.column_names['Ph√≤ng ban']),
                        ('ng∆∞·ªùi_th·ª±c_hi·ªán', self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán'])
                    ]:
                        if col_name in row and pd.notna(row[col_name]) and str(row[col_name]).strip():
                            result[field] = row[col_name]
                        else:
                            result[field] = ''
                    
                    direct_time_results.append(result)
            
            if direct_time_results:
                print(f"T√¨m ƒë∆∞·ª£c {len(direct_time_results)} k·∫øt qu·∫£ b·ªï sung cho '{time_unit}'")
                # L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£, t·ªëi ƒëa 10 k·∫øt qu·∫£
                results = direct_time_results[:max_results]
        
        # D·ªØ li·ªáu k·∫øt qu·∫£ cu·ªëi c√πng
        response = {
            'success': True,
            'query_info': query_info,
            'results': self.prepare_results_for_response(query, query_info, filtered_results) if not results else {'results': results}
        }
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON string ho·∫∑c dict t√πy theo y√™u c·∫ßu
        if kwargs.get('return_json', False):
            return json.dumps(response, ensure_ascii=False)
        return response
    
    def extract_role_terms(self, text):
        """Tr√≠ch xu·∫•t c√°c t·ª´ kh√≥a v·ªÅ vai tr√≤ t·ª´ vƒÉn b·∫£n"""
        # L·∫•y t·∫•t c·∫£ vai tr√≤ t·ª´ c·ªôt Ng∆∞·ªùi th·ª±c hi·ªán
        all_roles = set()
        role_col = self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']
        
        for val in self.df[role_col].dropna().unique():
            if isinstance(val, str):
                roles = [r.strip().lower() for r in val.split(',')]
                all_roles.update(roles)
        
        # Ki·ªÉm tra t·ª´ng vai tr√≤ c√≥ trong vƒÉn b·∫£n kh√¥ng
        found_roles = []
        for role in all_roles:
            if role in text.lower():
                found_roles.append(role)
        
        return found_roles
    
    def analyze_query(self, query):
        """Ph√¢n t√≠ch truy v·∫•n v√† x√°c ƒë·ªãnh lo·∫°i truy v·∫•n"""
        import re
        import unidecode
        
        query_info = {
            'original': query,
            'processed': query.lower(),
            'query_type': 'general',
            'search_focus': [],
            'components': {
                'giai_doan': None,
                'phong_ban': None,
                'cong_viec': None,
                'keywords': []
            }
        }
        
        # Ph√°t hi·ªán truy v·∫•n v·ªÅ ng∆∞·ªùi/nh√¢n s·ª±
        person_terms = ['ai', 'ng∆∞·ªùi', 'nh√¢n vi√™n', 'nh√¢n s·ª±', 'th·ª±c hi·ªán', 'ph·ª• tr√°ch', 'leader', 'team']
        
        # Ph√°t hi·ªán truy v·∫•n v·ªÅ quy tr√¨nh
        process_terms = ['giai ƒëo·∫°n', 'quy tr√¨nh', 'c√¥ng vi·ªác', 'ph√≤ng ban', 'b·ªô ph·∫≠n', 'thu·ªôc']
        
        # Ph√°t hi·ªán truy v·∫•n v·ªÅ m·ª•c ti√™u/k·∫øt qu·∫£
        content_terms = ['m·ª•c ti√™u', 'l√†m g√¨', 'k·∫øt qu·∫£', 'nhi·ªám v·ª•', 'tr√°ch nhi·ªám', 'c√¥ng vi·ªác', 'output']
        
        # Ph√°t hi·ªán truy v·∫•n v·ªÅ th·ªùi gian v√† t·∫ßn su·∫•t
        time_terms = ['h√†ng ng√†y', 'ng√†y', 'tu·∫ßn', 'th√°ng', 'qu√Ω', 'nƒÉm', 'ƒë·ªãnh k·ª≥', 'th∆∞·ªùng xuy√™n', 
                     't·∫ßn su·∫•t', 'khi n√†o', 'bao l√¢u', 'th·ªùi gian', 'duration', 'always on']
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ kh√≥a theo t·ª´ng lo·∫°i
        person_count = sum(1 for term in person_terms if term in query.lower())
        process_count = sum(1 for term in process_terms if term in query.lower())
        content_count = sum(1 for term in content_terms if term in query.lower())
        time_count = sum(1 for term in time_terms if term in query.lower())
        
        # X√°c ƒë·ªãnh m·ª•c ti√™u ch√≠nh c·ªßa truy v·∫•n
        if person_count > 0:
            query_info['search_focus'].append('person')
        if process_count > 0:
            query_info['search_focus'].append('process')
        if content_count > 0:
            query_info['search_focus'].append('content')
        if time_count > 0:
            query_info['search_focus'].append('time')
            query_info['components']['time_focus'] = True
            
            # Tr√≠ch xu·∫•t th√¥ng tin v·ªÅ ƒë∆°n v·ªã th·ªùi gian c·ª• th·ªÉ
            for term in time_terms:
                if term in query.lower():
                    query_info['components']['time_unit'] = term
                    break
        
        # N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c, t√¨m ki·∫øm t·ªïng qu√°t
        if not query_info['search_focus']:
            query_info['search_focus'] = ['process', 'content', 'person']
        
        # T·ª´ ƒëi·ªÉn bi·∫øn th·ªÉ m·ªü r·ªông
        common_variations = {
            'telesale': 'telesales',
            'tele sale': 'telesales',
            'phone sale': 'telesales',
            'marketing': 'mkt',
            'kinh doanh': 'sales',
            'kd': 'sales', 
            'mkt': 'marketing',
            'ti·∫øp th·ªã': 'marketing',
            'branding': 'branding mkt',
            'data': 'data qualification',
            'ngu·ªìn': 'sales sourcing',
            'thi·∫øt k·∫ø': 'design',
            't√¨m ki·∫øm': 'sales sourcing',
            'x√°c ƒë·ªãnh': 'data qualification',
            'ti·∫øp c·∫≠n': 'approach',
            'kh√°ch h√†ng': 'customer',
            'ƒë·∫°i l√Ω': 'agency',
            'nh√¢n s·ª±': 'hr',
            'nh√¢n l·ª±c': 'hr',
            'tuy·ªÉn d·ª•ng': 'hr',
            'h√†ng th√°ng': 'th√°ng',
            'h√†ng tu·∫ßn': 'tu·∫ßn',
            'h√†ng nƒÉm': 'nƒÉm',
            'h√†ng qu√Ω': 'qu√Ω',
            'ƒë·ªãnh k·ª≥ th√°ng': 'th√°ng',
            'ƒë·ªãnh k·ª≥ tu·∫ßn': 'tu·∫ßn',
            'ƒë·ªãnh k·ª≥ nƒÉm': 'nƒÉm'
        }
        
        # Chu·∫©n h√≥a query
        processed_query = query.lower()
        for variation, standard in common_variations.items():
            pattern = r'\b' + variation + r'\b' 
            processed_query = re.sub(pattern, standard, processed_query)
        
        query_info['processed'] = processed_query
        
        # Tr√≠ch xu·∫•t vai tr√≤ t·ª´ truy v·∫•n
        role_terms = self.extract_role_terms(processed_query)
        
        # X·ª≠ l√Ω chung cho t·∫•t c·∫£ vai tr√≤, kh√¥ng ch·ªâ Telesales
        for role in role_terms:
            query_info['components']['nguoi_thuc_hien'] = role.capitalize()
            if 'person' not in query_info['search_focus']:
                query_info['search_focus'].append('person')
            
            # N·∫øu truy v·∫•n li√™n quan ƒë·∫øn giai ƒëo·∫°n c·ªßa vai tr√≤
            if any(term in query.lower() for term in ['thu·ªôc', 'giai ƒëo·∫°n', 'l√†m vi·ªác trong']):
                # X·ª≠ l√Ω t√¨m giai ƒëo·∫°n t·ª´ b·∫£n ƒë·ªì √°nh x·∫°
                if role in self.role_stage_mapping:
                    if 'stage_keywords' not in query_info['components']:
                        query_info['components']['stage_keywords'] = []
                    
                    for stage in self.role_stage_mapping[role]:
                        query_info['components']['stage_keywords'].append(stage.lower())
                        # N·∫øu ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c giai ƒëo·∫°n, l·∫•y giai ƒëo·∫°n ƒë·∫ßu ti√™n
                        if not query_info['components']['giai_doan']:
                            query_info['components']['giai_doan'] = stage
                            
                    # ƒê·∫£m b·∫£o t√¨m ki·∫øm theo quy tr√¨nh
                    if 'process' not in query_info['search_focus']:
                        query_info['search_focus'].append('process')
                    
                    # Th√™m c√°c t·ª´ kh√≥a giai ƒëo·∫°n v√†o t·ª´ kh√≥a t√¨m ki·∫øm
                    for stage_kw in query_info['components']['stage_keywords']:
                        words = stage_kw.split()
                        for word in words:
                            if len(word) > 3 and word not in query_info['components']['keywords']:
                                query_info['components']['keywords'].append(word)
        
        # T√¨m c√°c giai ƒëo·∫°n trong c√¢u h·ªèi n·∫øu ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c
        if not query_info['components']['giai_doan']:
            stages = list(self.df[self.column_names['Giai ƒëo·∫°n']].dropna().unique()) 
            for stage in stages:
                if not isinstance(stage, str):
                    continue
                stage_lower = stage.lower()
                if stage_lower in processed_query or unidecode.unidecode(stage_lower) in unidecode.unidecode(processed_query):
                    query_info['components']['giai_doan'] = stage
                    break
        
        # T√¨m c√°c ph√≤ng ban trong c√¢u h·ªèi
        departments = list(self.df[self.column_names['Ph√≤ng ban']].dropna().unique())
        for dept in departments:
            if not isinstance(dept, str):
                continue
            dept_lower = dept.lower()
            if dept_lower in processed_query or unidecode.unidecode(dept_lower) in unidecode.unidecode(processed_query):
                query_info['components']['phong_ban'] = dept
                break
        
        # T√¨m c√°c th√¥ng tin v·ªÅ ƒë·ªãnh k·ª≥ th·ª±c hi·ªán
        if 'time_focus' in query_info['components'] and query_info['components']['time_focus']:
            time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
            periodic_values = list(self.df[time_col].dropna().unique())
            
            for period in periodic_values:
                if not isinstance(period, str):
                    continue
                period_lower = period.lower()
                if period_lower in processed_query or unidecode.unidecode(period_lower) in unidecode.unidecode(processed_query):
                    query_info['components']['dinh_ky'] = period
                    break
        
        # T√°ch c√°c t·ª´ kh√≥a ch√≠nh
        for word in processed_query.split():
            if len(word) > 3 and word not in query_info['components']['keywords']:
                query_info['components']['keywords'].append(word)
        
        return query_info
    
    def multi_vector_search(self, query_info, top_n=3):
        """T√¨m ki·∫øm ƒëa vector d·ª±a tr√™n nhi·ªÅu ti√™u ch√≠ v√† k·∫øt h·ª£p k·∫øt qu·∫£"""
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        # ƒêi·ªÅu ch·ªânh top_n d·ª±a v√†o lo·∫°i truy v·∫•n
        if 'time_focus' in query_info['components'] and query_info['components']['time_focus']:
            # TƒÉng s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ cho truy v·∫•n th·ªùi gian
            top_n = 10  # TƒÉng l√™n 10 k·∫øt qu·∫£ thay v√¨ m·∫∑c ƒë·ªãnh 3
            print(f"Truy v·∫•n th·ªùi gian: TƒÉng top_n l√™n {top_n}")
        
        results = []
        scores = []
        
        # T·∫°o vector truy v·∫•n cho c√°c lo·∫°i t√¨m ki·∫øm kh√°c nhau
        query_text = query_info['processed']
        
        # Tr·ªçng s·ªë cho t·ª´ng lo·∫°i t√¨m ki·∫øm - ƒë·ªông d·ª±a tr√™n lo·∫°i truy v·∫•n
        weights = {
            'title': 0.20,
            'person': 0.15,
            'process': 0.15,
            'content': 0.30,
            'time': 0.10,
            'additional': 0.10
        }
        
        # ƒêi·ªÅu ch·ªânh tr·ªçng s·ªë d·ª±a tr√™n lo·∫°i truy v·∫•n
        if 'time_focus' in query_info['components'] and query_info['components']['time_focus']:
            # TƒÉng tr·ªçng s·ªë th·ªùi gian n·∫øu truy v·∫•n li√™n quan ƒë·∫øn th·ªùi gian
            weights['time'] = 0.35
            weights['title'] = 0.15
            weights['content'] = 0.20
            weights['person'] = 0.10
            weights['process'] = 0.15
            weights['additional'] = 0.05
        
        # L·∫•y th√¥ng tin v·ªÅ vai tr√≤ v√† giai ƒëo·∫°n d·ª± ki·∫øn t·ª´ query_info
        role = None
        expected_stages = []
        if 'nguoi_thuc_hien' in query_info['components'] and query_info['components']['nguoi_thuc_hien']:
            role = query_info['components']['nguoi_thuc_hien'].lower()
            # L·∫•y danh s√°ch giai ƒëo·∫°n d·ª± ki·∫øn t·ª´ b·∫£n ƒë·ªì √°nh x·∫°
            if role in self.role_stage_mapping:
                expected_stages = self.role_stage_mapping[role]
        
        # T√¨m ki·∫øm d·ª±a tr√™n ti√™u ƒë·ªÅ/t√™n giai ƒëo·∫°n (t√¨m ki·∫øm t·ªïng h·ª£p)
        if 'process' in query_info['search_focus'] or not query_info['search_focus']:
            title_vector = self.main_vectorizer.transform([query_text])
            title_similarities = cosine_similarity(title_vector, self.main_matrix).flatten()
            
            # TƒÉng ƒëi·ªÉm cho c√°c giai ƒëo·∫°n d·ª± ki·∫øn c·ªßa vai tr√≤ n·∫øu c√≥
            if expected_stages and any(term in query_text for term in ['thu·ªôc', 'giai ƒëo·∫°n', 'l√†m vi·ªác']):
                for i, index in enumerate(self.df.index):
                    stage_col = self.column_names['Giai ƒëo·∫°n']
                    if isinstance(self.df.loc[index, stage_col], str):
                        for stage in expected_stages:
                            if stage.lower() in self.df.loc[index, stage_col].lower():
                                title_similarities[i] *= 1.5  # TƒÉng 50% ƒëi·ªÉm
            
            for i in range(len(self.df)):
                results.append(i)
                scores.append(title_similarities[i] * weights['title'])
        
        # T√¨m ki·∫øm d·ª±a tr√™n ng∆∞·ªùi th·ª±c hi·ªán
        if 'person' in query_info['search_focus']:
            person_vector = self.person_vectorizer.transform([query_text])
            person_similarities = cosine_similarity(person_vector, self.person_matrix).flatten()
            
            # TƒÉng ƒëi·ªÉm cho h√†ng c√≥ vai tr√≤ ph√π h·ª£p
            if role:
                role_col = self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']
                stage_col = self.column_names['Giai ƒëo·∫°n']
                
                for i, index in enumerate(self.df.index):
                    if isinstance(self.df.loc[index, role_col], str) and role in self.df.loc[index, role_col].lower():
                        person_similarities[i] *= 1.8  # TƒÉng 80% ƒëi·ªÉm
                    
                    # N·∫øu ƒëang t√¨m ki·∫øm giai ƒëo·∫°n c·ªßa vai tr√≤
                    if expected_stages and any(term in query_text for term in ['thu·ªôc', 'giai ƒëo·∫°n', 'l√†m vi·ªác']):
                        # Gi·∫£m ƒëi·ªÉm cho c√°c giai ƒëo·∫°n kh√¥ng ph√π h·ª£p v·ªõi vai tr√≤
                        if (isinstance(self.df.loc[index, stage_col], str) and 
                            not any(stage.lower() in self.df.loc[index, stage_col].lower() for stage in expected_stages)):
                            person_similarities[i] *= 0.4  # Gi·∫£m 60% ƒëi·ªÉm
            
            for i in range(len(self.df)):
                if i < len(results) and results[i] == i:
                    scores[i] += person_similarities[i] * weights['person']
                else:
                    results.append(i)
                    scores.append(person_similarities[i] * weights['person'])
        
        # T√¨m ki·∫øm d·ª±a tr√™n ph√≤ng ban & quy tr√¨nh
        if 'process' in query_info['search_focus']:
            dept_vector = self.process_vectorizer.transform([query_text])
            dept_similarities = cosine_similarity(dept_vector, self.process_matrix).flatten()
            
            for i in range(len(self.df)):
                if i < len(results) and results[i] == i:
                    scores[i] += dept_similarities[i] * weights['process']
                else:
                    results.append(i)
                    scores.append(dept_similarities[i] * weights['process'])
        
        # T√¨m ki·∫øm d·ª±a tr√™n n·ªôi dung
        if 'content' in query_info['search_focus'] or not query_info['search_focus']:
            content_vector = self.content_vectorizer.transform([query_text])
            content_similarities = cosine_similarity(content_vector, self.content_matrix).flatten()
            
            # TƒÉng ƒëi·ªÉm cho c√°c giai ƒëo·∫°n d·ª± ki·∫øn c·ªßa vai tr√≤ n·∫øu c√≥
            if role and expected_stages and 'm·ª•c ti√™u' in query_text:
                role_col = self.column_names['Ng∆∞·ªùi th·ª±c hi·ªán']
                stage_col = self.column_names['Giai ƒëo·∫°n']
                
                for i, index in enumerate(self.df.index):
                    # N·∫øu h√†ng v·ª´a c√≥ giai ƒëo·∫°n ph√π h·ª£p v·ª´a c√≥ vai tr√≤ ph√π h·ª£p
                    if (isinstance(self.df.loc[index, stage_col], str) and 
                        any(stage.lower() in self.df.loc[index, stage_col].lower() for stage in expected_stages) and
                        isinstance(self.df.loc[index, role_col], str) and
                        role in self.df.loc[index, role_col].lower()):
                        content_similarities[i] *= 2.0  # TƒÉng g·∫•p ƒë√¥i ƒëi·ªÉm
            
            for i in range(len(self.df)):
                if i < len(results) and results[i] == i:
                    scores[i] += content_similarities[i] * weights['content']
                else:
                    results.append(i)
                    scores.append(content_similarities[i] * weights['content'])
        
        # T√¨m ki·∫øm d·ª±a tr√™n th√¥ng tin th·ªùi gian
        if 'time' in query_info['search_focus'] or 'time_focus' in query_info['components']:
            time_vector = self.time_vectorizer.transform([query_text])
            time_similarities = cosine_similarity(time_vector, self.time_matrix).flatten()
            
            # TƒÉng ƒëi·ªÉm cho k·∫øt qu·∫£ ph√π h·ª£p v·ªõi ƒë∆°n v·ªã th·ªùi gian c·ª• th·ªÉ
            if 'time_unit' in query_info['components'] and query_info['components']['time_unit']:
                time_unit = query_info['components']['time_unit']
                time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
                
                for i, index in enumerate(self.df.index):
                    if isinstance(self.df.loc[index, time_col], str) and time_unit.lower() in self.df.loc[index, time_col].lower():
                        time_similarities[i] *= 3.0  # TƒÉng m·∫°nh ƒëi·ªÉm s·ªë khi kh·ªõp ƒë√∫ng ƒë∆°n v·ªã th·ªùi gian
            
            # TƒÉng ƒëi·ªÉm cho k·∫øt qu·∫£ ph√π h·ª£p v·ªõi ƒë·ªãnh k·ª≥ c·ª• th·ªÉ
            if 'dinh_ky' in query_info['components'] and query_info['components']['dinh_ky']:
                specified_period = query_info['components']['dinh_ky']
                time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
                
                for i, index in enumerate(self.df.index):
                    if isinstance(self.df.loc[index, time_col], str) and specified_period.lower() in self.df.loc[index, time_col].lower():
                        time_similarities[i] *= 5.0  # TƒÉng r·∫•t m·∫°nh ƒëi·ªÉm s·ªë khi kh·ªõp ch√≠nh x√°c ƒë·ªãnh k·ª≥
            
            for i in range(len(self.df)):
                if i < len(results) and results[i] == i:
                    scores[i] += time_similarities[i] * weights['time']
                else:
                    results.append(i)
                    scores.append(time_similarities[i] * weights['time'])
        
        # T√¨m ki·∫øm d·ª±a tr√™n th√¥ng tin b·ªï sung
        if 'additional' in query_info['search_focus'] or not query_info['search_focus']:
            additional_vector = self.additional_vectorizer.transform([query_text])
            additional_similarities = cosine_similarity(additional_vector, self.additional_matrix).flatten()
            
            for i in range(len(self.df)):
                if i < len(results) and results[i] == i:
                    scores[i] += additional_similarities[i] * weights['additional']
                else:
                    results.append(i)
                    scores.append(additional_similarities[i] * weights['additional'])
        
        # T√¨m ki·∫øm tr·ª±c ti·∫øp trong t·ª´ng c·ªôt n·∫øu c·∫ßn
        if 'time_focus' in query_info['components'] or role or 'dinh_ky' in query_info['components']:
            # Danh s√°ch c·ªôt c·∫ßn t√¨m ki·∫øm tr·ª±c ti·∫øp
            columns_to_search = []
            
            if 'time_focus' in query_info['components']:
                columns_to_search.append('ƒê·ªãnh k·ª≥ th·ª±c hi·ªán')
                
            if role:
                columns_to_search.append('Ng∆∞·ªùi th·ª±c hi·ªán')
                
            for col_name in columns_to_search:
                if col_name in self.column_vectorizers:
                    # T·∫°o vector truy v·∫•n cho c·ªôt c·ª• th·ªÉ
                    col_vector = self.column_vectorizers[col_name].transform([query_text])
                    col_similarities = cosine_similarity(col_vector, self.column_matrices[col_name]).flatten()
                    
                    # C·∫≠p nh·∫≠t ƒëi·ªÉm s·ªë
                    for i in range(len(self.df)):
                        if i < len(results) and results[i] == i:
                            # TƒÉng c∆∞·ªùng ƒëi·ªÉm s·ªë cho t√¨m ki·∫øm tr·ª±c ti·∫øp
                            scores[i] += col_similarities[i] * 0.15
                        else:
                            results.append(i)
                            scores.append(col_similarities[i] * 0.15)
        
        # K·∫øt h·ª£p k·∫øt qu·∫£
        result_dict = {}
        for i, idx in enumerate(results):
            if idx not in result_dict or scores[i] > result_dict[idx]:
                result_dict[idx] = scores[i]
        
        # S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒëi·ªÉm s·ªë
        sorted_results = sorted([(idx, score) for idx, score in result_dict.items()], 
                                key=lambda x: x[1], reverse=True)
        
        # X·ª≠ l√Ω sau c√πng cho t√¨m ki·∫øm theo th·ªùi gian
        if 'time_focus' in query_info['components'] and query_info['components']['time_focus']:
            time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
            time_unit = query_info['components'].get('time_unit', '')
            
            # T·∫°o danh s√°ch k·∫øt qu·∫£ ph√π h·ª£p v·ªõi ƒë∆°n v·ªã th·ªùi gian
            time_matching_results = []
            other_results = []
            
            for idx, score in sorted_results:
                if (isinstance(self.df.iloc[idx][time_col], str) and 
                    time_unit and time_unit.lower() in self.df.iloc[idx][time_col].lower()):
                    # TƒÉng ƒëi·ªÉm cho k·∫øt qu·∫£ ph√π h·ª£p
                    time_matching_results.append((idx, score * 1.2))
                else:
                    other_results.append((idx, score))
            
            # K·∫øt h·ª£p k·∫øt qu·∫£, ∆∞u ti√™n k·∫øt qu·∫£ ph√π h·ª£p th·ªùi gian
            sorted_results = sorted(time_matching_results, key=lambda x: x[1], reverse=True)
            
            # Th√™m c√°c k·∫øt qu·∫£ kh√°c n·∫øu c·∫ßn
            if len(sorted_results) < top_n:
                remaining_slots = top_n - len(sorted_results)
                sorted_results.extend(other_results[:remaining_slots])
            
            print(f"T√¨m ƒë∆∞·ª£c {len(time_matching_results)} k·∫øt qu·∫£ ph√π h·ª£p v·ªõi ƒë∆°n v·ªã th·ªùi gian '{time_unit}'")
        
        # Lo·∫°i b·ªè c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm s·ªë qu√° th·∫•p
        filtered_results = [(idx, score) for idx, score in sorted_results if score > 0.15]
        
        print(f"S·ªë k·∫øt qu·∫£ sau khi l·ªçc: {len(filtered_results)}, top_n={top_n}")
        
        # Tr·∫£ v·ªÅ top N k·∫øt qu·∫£
        return filtered_results[:top_n]
    
    def filter_and_rank_results(self, scores, min_threshold=0.15):
        """L·ªçc v√† x·∫øp h·∫°ng k·∫øt qu·∫£ d·ª±a tr√™n ƒëi·ªÉm s·ªë"""
        import numpy as np
        
        # L·ªçc c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm th·∫•p
        valid_indices = np.where(scores > min_threshold)[0]
        
        if len(valid_indices) == 0:
            return []
        
        # T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho m·ªói h√†ng
        quality_scores = np.zeros(len(scores))
        
        for idx in valid_indices:
                row = self.df.iloc[idx]
            # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu: c√≥ bao nhi√™u tr∆∞·ªùng c√≥ gi√° tr·ªã
            non_empty_fields = sum(1 for x in row if pd.notna(x) and str(x).strip() != '')
            quality_score = non_empty_fields / len(row)
            
            # Ki·ªÉm tra xem d√≤ng n√†y c√≥ ph·∫£i l√† header kh√¥ng
            # N·∫øu l√† header, ƒëi·ªÉm ch·∫•t l∆∞·ª£ng s·∫Ω r·∫•t th·∫•p
            if self.is_likely_header_row(row):
                quality_score = 0.01
                
            quality_scores[idx] = quality_score
        
        # K·∫øt h·ª£p ƒëi·ªÉm s·ªë t∆∞∆°ng ƒë·ªìng v√† ch·∫•t l∆∞·ª£ng
        final_scores = scores * 0.7 + quality_scores * 0.3
        
        # L·∫•y top 5 k·∫øt qu·∫£
        top_indices = final_scores.argsort()[-5:][::-1]
        return [(i, final_scores[i]) for i in top_indices if final_scores[i] > min_threshold]
    
    def is_likely_header_row(self, row):
        """Ki·ªÉm tra m·ªôt h√†ng c√≥ kh·∫£ nƒÉng l√† header kh√¥ng"""
        # ƒê·∫øm s·ªë l∆∞·ª£ng gi√° tr·ªã c√≥ ƒë·∫∑c ƒëi·ªÉm c·ªßa header
        header_like_count = 0
        value_count = 0
        
        for col, val in row.items():
            if pd.isna(val) or str(val).strip() == '':
                continue
                
            value_count += 1
            val_str = str(val)
            
            # Ki·ªÉm tra c√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa header
            if (val_str.istitle() or val_str.isupper() or 
                val_str in ['A', 'R', 'BOD', 'CEO'] or
                val_str in ['Giai ƒëo·∫°n', 'Ph√≤ng ban', 'M·ª•c ti√™u']):
                header_like_count += 1
        
        # N·∫øu >60% gi√° tr·ªã c√≥ ƒë·∫∑c ƒëi·ªÉm c·ªßa header
        return value_count > 0 and header_like_count / value_count > 0.6
    
    def is_likely_header_value(self, value):
        """Ki·ªÉm tra m·ªôt gi√° tr·ªã c√≥ kh·∫£ nƒÉng l√† gi√° tr·ªã header kh√¥ng"""
        if not isinstance(value, str):
            return False
            
        # C√°c chu·ªói header ƒëi·ªÉn h√¨nh
        header_values = ['Giai ƒëo·∫°n', 'C√¥ng vi·ªác', 'Ph√≤ng ban', 'M·ª•c ti√™u',
                         'A', 'R', 'L√†m g√¨', 'K·∫øt qu·∫£ tr·∫£ ra', 'Duration']
        
        if value in header_values:
            return True
            
        # Ki·ªÉm tra n·∫øu vi·∫øt hoa v√† ƒë·ªô d√†i ng·∫Øn
        if value.isupper() and len(value) < 15:
            return True
            
        # Ki·ªÉm tra Title Case cho c√°c t·ª´ ng·∫Øn
        if value.istitle() and len(value.split()) <= 3:
            return True
            
        return False
    
    def prepare_results_for_response(self, original_query, query_info, ranked_indices_scores):
        """Chu·∫©n b·ªã k·∫øt qu·∫£ cu·ªëi c√πng cho ph·∫£n h·ªìi"""
        import json
        
        if not ranked_indices_scores:
            suggestions = []
            
            # ƒê·ªÅ xu·∫•t giai ƒëo·∫°n
            if not query_info['components']['giai_doan']:
                stages = [str(s) for s in self.df[self.column_names['Giai ƒëo·∫°n']].dropna().unique() if isinstance(s, str) and str(s).strip()]
                if stages:
                    sample_stages = ", ".join(stages[:3]) + "..."
                    suggestions.append(f"th√™m giai ƒëo·∫°n c·ª• th·ªÉ (v√≠ d·ª•: {sample_stages})")
            
            # ƒê·ªÅ xu·∫•t ph√≤ng ban
            if not query_info['components']['phong_ban']:
                depts = [str(d) for d in self.df[self.column_names['Ph√≤ng ban']].dropna().unique() if isinstance(d, str) and str(d).strip()]
                if depts:
                    sample_depts = ", ".join(depts[:3]) + "..."
                    suggestions.append(f"th√™m ph√≤ng ban c·ª• th·ªÉ (v√≠ d·ª•: {sample_depts})")
            
            # ƒê·ªÅ xu·∫•t th·ªùi gian n·∫øu ƒëang t√¨m ki·∫øm theo th·ªùi gian
            if 'time_focus' in query_info['components']:
                time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
                periods = [str(p) for p in self.df[time_col].dropna().unique() if isinstance(p, str) and str(p).strip()]
                if periods:
                    sample_periods = ", ".join(periods[:3]) + "..."
                    suggestions.append(f"th√™m ƒë·ªãnh k·ª≥ th·ª±c hi·ªán c·ª• th·ªÉ (v√≠ d·ª•: {sample_periods})")
            
            suggest_text = ""
            if suggestions:
                suggest_text = f"B·∫°n c√≥ th·ªÉ th·ª≠ {' ho·∫∑c '.join(suggestions)}."
            
            return {
                "query_info": query_info,
                "message": f"Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n. {suggest_text}",
                "total_results": 0,
                "results": []
            }
        
        results = []
        for idx, score in ranked_indices_scores:
            row = self.df.iloc[idx]
            
            # Ki·ªÉm tra xem h√†ng c√≥ ph·∫£i l√† header/ti√™u ƒë·ªÅ hay kh√¥ng
            if self.is_likely_header_row(row):
                continue
            
            # ƒê√°nh gi√° ƒë·ªô ph√π h·ª£p
            relevance = "Cao" if score > 0.5 else "Trung b√¨nh" if score > 0.3 else "Th·∫•p"
            
            # Chuy·ªÉn ƒë·ªïi row th√†nh dictionary c√≥ c·∫•u tr√∫c
                result = {
                "do_phu_hop": relevance,
                "diem_so": round(score, 2)
            }
            
            # Th√™m c√°c tr∆∞·ªùng d·ªØ li·ªáu c√≥ √Ω nghƒ©a
            for col, val in row.items():
                if pd.notna(val) and str(val).strip():
                    # S·ª≠ d·ª•ng t√™n th√¢n thi·ªán n·∫øu c√≥, n·∫øu kh√¥ng th√¨ gi·ªØ nguy√™n t√™n c·ªôt
                    friendly_name = self.inverse_column_names.get(col, col)
                    # Ki·ªÉm tra n·∫øu gi√° tr·ªã l√† header/ti√™u ƒë·ªÅ
                    if not self.is_likely_header_value(val):
                        result[friendly_name.lower().replace(' ', '_')] = val
            
            # Ki·ªÉm tra n·∫øu ƒë√¢y l√† truy v·∫•n li√™n quan ƒë·∫øn th·ªùi gian, ƒë·∫£m b·∫£o tr∆∞·ªùng th·ªùi gian ƒë∆∞·ª£c bao g·ªìm
            if 'time_focus' in query_info['components']:
                time_col = self.column_names['ƒê·ªãnh k·ª≥ th·ª±c hi·ªán']
                time_val = row[time_col]
                if pd.notna(time_val) and str(time_val).strip():
                    result['ƒë·ªãnh_k·ª≥_th·ª±c_hi·ªán'] = time_val
                    
                    # TƒÉng th√™m ƒëi·ªÉm n·∫øu th·ªùi gian kh·ªõp v·ªõi y√™u c·∫ßu
                    if 'time_unit' in query_info['components']:
                        time_unit = query_info['components']['time_unit']
                        if time_unit and time_unit.lower() in str(time_val).lower():
                            # ƒêi·ªÅu ch·ªânh ƒë·ªô ph√π h·ª£p
                            result['do_phu_hop'] = "Cao"
                            result['diem_so'] = round(min(0.95, score * 1.3), 2)
            
                results.append(result)
        
        # S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒë·ªô ph√π h·ª£p n·∫øu t√¨m theo th·ªùi gian
        if 'time_focus' in query_info['components'] and 'time_unit' in query_info['components']:
            time_unit = query_info['components']['time_unit']
            
            # S·∫Øp x·∫øp ƒë·ªÉ ∆∞u ti√™n c√°c k·∫øt qu·∫£ c√≥ th·ªùi gian kh·ªõp
            def time_sort_key(result):
                # N·∫øu c√≥ tr∆∞·ªùng ƒë·ªãnh_k·ª≥_th·ª±c_hi·ªán v√† n√≥ ch·ª©a time_unit
                if 'ƒë·ªãnh_k·ª≥_th·ª±c_hi·ªán' in result and time_unit in str(result['ƒë·ªãnh_k·ª≥_th·ª±c_hi·ªán']).lower():
                    return (1, result['diem_so'])  # ∆Øu ti√™n cao nh·∫•t
                return (0, result['diem_so'])      # ∆Øu ti√™n th·∫•p h∆°n
                
            results = sorted(results, key=time_sort_key, reverse=True)
        
        return {
            "query_info": query_info,
            "total_results": len(results),
            "results": results
        }

# ‚úÖ Ch·ª©c nƒÉng l∆∞u v√† n·∫°p l·ªãch s·ª≠ h·ªôi tho·∫°i
def save_conversation(messages, conversation_name=None, path="./history"):
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    if not os.path.exists(path):
        os.makedirs(path)
    
    # T·∫°o t√™n file t·ª´ th·ªùi gian ho·∫∑c t√™n ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if conversation_name:
        # L√†m s·∫°ch t√™n file
        conversation_name = ''.join(c for c in conversation_name if c.isalnum() or c in ' -_')
        filename = f"{path}/{conversation_name}.json"
    else:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{path}/chat_{timestamp}.json"
    
    # L∆∞u h·ªôi tho·∫°i v√†o file
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)
    
    return filename

def load_conversation(filename_or_index, path="./history"):
    # Ki·ªÉm tra n·∫øu th∆∞ m·ª•c t·ªìn t·∫°i
    if not os.path.exists(path):
        return None, "Ch∆∞a c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i n√†o ƒë∆∞·ª£c l∆∞u."
    
    # L·∫•y danh s√°ch file
    files = [f for f in os.listdir(path) if f.endswith('.json')]
    
    if not files:
        return None, "Kh√¥ng t√¨m th·∫•y file l·ªãch s·ª≠ h·ªôi tho·∫°i n√†o."
    
    # S·∫Øp x·∫øp theo th·ªùi gian t·∫°o (m·ªõi nh·∫•t tr∆∞·ªõc)
    files.sort(key=lambda x: os.path.getmtime(os.path.join(path, x)), reverse=True)
    
    filename = None
    # N·∫øu l√† s·ªë, l·∫•y file theo index
    if str(filename_or_index).isdigit():
        index = int(filename_or_index)
        if 0 <= index < len(files):
            filename = os.path.join(path, files[index])
        else:
            return None, f"Ch·ªâ s·ªë kh√¥ng h·ª£p l·ªá. Vui l√≤ng ch·ªçn t·ª´ 0 ƒë·∫øn {len(files)-1}."
    else:
        # N·∫øu l√† t√™n file, t√¨m file ph√π h·ª£p
        for file in files:
            if filename_or_index in file:
                filename = os.path.join(path, file)
                break
    
    if not filename:
        return None, f"Kh√¥ng t√¨m th·∫•y file '{filename_or_index}'. S·ª≠ d·ª•ng '/list' ƒë·ªÉ xem danh s√°ch h·ªôi tho·∫°i."
    
    # ƒê·ªçc file
    try:
        with open(filename, "r", encoding="utf-8") as f:
            messages = json.load(f)
        return messages, f"ƒê√£ t·∫£i l·ªãch s·ª≠ h·ªôi tho·∫°i t·ª´ {filename}"
    except Exception as e:
        return None, f"L·ªói khi ƒë·ªçc file: {str(e)}"

def list_conversations(path="./history"):
    if not os.path.exists(path):
        return "Ch∆∞a c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i n√†o ƒë∆∞·ª£c l∆∞u."
    
    files = [f for f in os.listdir(path) if f.endswith('.json')]
    
    if not files:
        return "Kh√¥ng t√¨m th·∫•y file l·ªãch s·ª≠ h·ªôi tho·∫°i n√†o."
    
    # S·∫Øp x·∫øp theo th·ªùi gian t·∫°o (m·ªõi nh·∫•t tr∆∞·ªõc)
    files.sort(key=lambda x: os.path.getmtime(os.path.join(path, x)), reverse=True)
    
    result = "Danh s√°ch h·ªôi tho·∫°i ƒë√£ l∆∞u:\n"
    for i, file in enumerate(files):
        # L·∫•y th·ªùi gian t·∫°o
        timestamp = datetime.datetime.fromtimestamp(os.path.getmtime(os.path.join(path, file)))
        result += f"{i}: {file} - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n"
    
    result += "\nS·ª≠ d·ª•ng '/load [s·ªë th·ª© t·ª±]' ho·∫∑c '/load [t√™n file]' ƒë·ªÉ t·∫£i l·ªãch s·ª≠."
    return result

# ‚úÖ Step 2: C·∫•u h√¨nh LLM
llm_cfg = {
    'model': 'qwen3-30b-a3b',
    'model_server': 'http://localhost:1234/v1',  # LM Studio m·∫∑c ƒë·ªãnh
    'api_key': 'EMPTY',
}

# ‚úÖ Step 3: T·∫°o Assistant
assistant = Assistant(
    llm=llm_cfg,
    function_list=["tra_cuu_quy_trinh"],
    system_message="""B·∫°n l√† chuy√™n gia v·ªÅ m√¥ h√¨nh RACI c·ªßa c√¥ng ty, gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ quy tr√¨nh c√¥ng vi·ªác d·ª±a tr√™n m√¥ h√¨nh RACI.

Nguy√™n t·∫Øc l√†m vi·ªác:
1. Lu√¥n g·ªçi tool tra_cuu_quy_trinh khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ quy tr√¨nh c√¥ng vi·ªác
2. N·∫øu nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu JSON, h√£y bi·∫øn ƒë·ªïi n√≥ th√†nh vƒÉn b·∫£n c√≥ c·∫•u tr√∫c r√µ r√†ng, d·ªÖ ƒë·ªçc
3. N·∫øu nhi·ªÅu k·∫øt qu·∫£, h√£y t√≥m t·∫Øt ƒëi·ªÉm chung v√† n√™u r√µ s·ª± kh√°c bi·ªát
4. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y g·ª£i √Ω ng∆∞·ªùi d√πng cung c·∫•p th√™m chi ti·∫øt v·ªÅ giai ƒëo·∫°n, ph√≤ng ban, ho·∫∑c c√¥ng vi·ªác c·ª• th·ªÉ
5. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch, c√≥ c·∫•u tr√∫c r√µ r√†ng
6. N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ tool, th√¨ h√£y n√≥i r√µ m√¨nh kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu v√† mu·ªën tr·∫£ l·ªùi theo ki·∫øn th·ª©c c·ªßa m√¥ h√¨nh kh√¥ng.

Khi c·∫ßn bi·∫øt th√™m th√¥ng tin, h√£y h·ªèi v·ªÅ:
- Giai ƒëo·∫°n quy tr√¨nh: Branding MKT, Sales Sourcing, Data Qualification, Approach, v.v.
- Ph√≤ng ban: Marketing, Kinh doanh, v.v.
- Lo·∫°i c√¥ng vi·ªác c·ª• th·ªÉ
"""
)

# ‚úÖ Step 4: Giao di·ªán chat ƒë∆°n gi·∫£n
messages = []
max_history = 6  # 3 c·∫∑p h·ªôi tho·∫°i (3 user + 3 assistant)

# Hi·ªÉn th·ªã th√¥ng tin ch√†o m·ª´ng
print("\n" + "="*50)
print("ü§ñ CHATBOT QUY TR√åNH C√îNG VI·ªÜC")
print("="*50)
print("C√°c l·ªánh ƒë·∫∑c bi·ªát:")
print("/save [t√™n] - L∆∞u h·ªôi tho·∫°i hi·ªán t·∫°i")
print("/load [s·ªë|t√™n] - T·∫£i l·ªãch s·ª≠ h·ªôi tho·∫°i")
print("/list - Xem danh s√°ch h·ªôi tho·∫°i ƒë√£ l∆∞u")
print("/clear - X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i hi·ªán t·∫°i")
print("/exit - Tho√°t ch∆∞∆°ng tr√¨nh")
print("="*50 + "\n")

while True:
    query = input("‚ùì C√¢u h·ªèi: ").strip()
    if not query:
        continue
        
    # X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát
    if query.startswith("/"):
        cmd_parts = query.split(maxsplit=1)
        cmd = cmd_parts[0].lower()
        
        # L·ªánh tho√°t
        if cmd == "/exit":
            print("üëã T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i.")
            break
            
        # L·ªánh l∆∞u h·ªôi tho·∫°i
        elif cmd == "/save":
            name = cmd_parts[1] if len(cmd_parts) > 1 else None
            saved_file = save_conversation(messages, name)
            print(f"ü§ñ ƒê√£ l∆∞u h·ªôi tho·∫°i v√†o {saved_file}")
            continue
            
        # L·ªánh t·∫£i h·ªôi tho·∫°i
        elif cmd == "/load":
            if len(cmd_parts) < 2:
                print("ü§ñ Vui l√≤ng cung c·∫•p s·ªë th·ª© t·ª± ho·∫∑c t√™n file. S·ª≠ d·ª•ng /list ƒë·ªÉ xem danh s√°ch.")
            else:
                loaded_messages, message = load_conversation(cmd_parts[1])
                if loaded_messages:
                    messages = loaded_messages
                    print(f"ü§ñ {message}")
                else:
                    print(f"ü§ñ {message}")
            continue
            
        # L·ªánh li·ªát k√™ h·ªôi tho·∫°i
        elif cmd == "/list":
            conversations_list = list_conversations()
            print(f"ü§ñ {conversations_list}")
            continue
            
        # L·ªánh x√≥a l·ªãch s·ª≠
        elif cmd == "/clear":
            messages = []
            print("ü§ñ ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i hi·ªán t·∫°i.")
            continue
    
    # X·ª≠ l√Ω c√¢u h·ªèi b√¨nh th∆∞·ªùng
    messages.append({'role': 'user', 'content': query})

    # Hi·ªÉn th·ªã ƒëang x·ª≠ l√Ω
    loading_chars = "|/-\\"
    for i in range(5):
        print(f"\rü§ñ ƒêang suy nghƒ© {loading_chars[i % len(loading_chars)]}", end='', flush=True)
        time.sleep(0.2)
    print("\rü§ñ Tr·∫£ l·ªùi: ", end='', flush=True)
    
    # G·ªçi tr·ª£ l√Ω v√† x·ª≠ l√Ω ph·∫£n h·ªìi
    response = []
    response_text = ''
    for r in assistant.run(messages=messages):
        chunk = r[-1]['content']
        print(chunk, end='', flush=True)
        response_text += chunk
        response.append(r[-1])
    messages.extend(response)
    
    # Gi·ªØ l·∫°i l·ªãch s·ª≠ theo c·∫∑p h·ªôi tho·∫°i
    if len(messages) > max_history:
        # Ch·ªâ gi·ªØ l·∫°i c√°c c·∫∑p ho√†n ch·ªânh, b·∫Øt ƒë·∫ßu v·ªõi user
        # T√¨m c√°c c·∫∑p ho√†n ch·ªânh t·ª´ cu·ªëi l√™n
        new_messages = []
        pairs_count = 0
        i = len(messages) - 2  # B·∫Øt ƒë·∫ßu t·ª´ c·∫∑p cu·ªëi c√πng
        
        while i >= 0 and pairs_count < (max_history // 2):
            if messages[i]['role'] == 'user' and messages[i+1]['role'] == 'assistant':
                new_messages = [messages[i], messages[i+1]] + new_messages
                pairs_count += 1
                i -= 2
            else:
                i -= 1
                
        # N·∫øu tin nh·∫Øn cu·ªëi l√† user v√† ch∆∞a ƒë∆∞·ª£c tr·∫£ l·ªùi
        if messages and messages[-1]['role'] == 'user' and messages[-1] not in new_messages:
            new_messages.append(messages[-1])
            
        messages = new_messages
    
    print()  # xu·ªëng d√≤ng sau tr·∫£ l·ªùi